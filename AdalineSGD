import numpy as np
import pandas as pan

class AdalineSGD:
    def __init__(self, tr_inpt, labels, epoch, Lr):
        #Intialize parameters
        self.epoch = epoch
        self.tr_inpt = tr_inpt
       
        if self.tr_inpt.ndim == 1:#If self.tr_input.shape == (a, ) where a member of integer set
            self.sz = 1 #For 1D array there is 1 column = column length/width is 1 rather than nothing
        else:
            self.sz = self.tr_inpt.shape[1] #Length of row (number of features per sample)
        #.shape returns a tuple and element at index 1 indicates the length of the row
        
        self.w = self.weights() #weights 
        self.Lr = Lr #Learning rate
        self.labels = labels #Training data labels
        
        self.learning() #Learning algorithm execution
        self.plotErrors_Cost() #Generate log(cost) vs epoch graph
    
        
    def weights(self):
        #where sz is the size of x (number of x)
        self.w = np.random.random(self.sz+1)

        #random.randfl ? generate float of random wieght
        return self.w
    
    def z(self, x):
        #generate dot product between w and features x
        return np.dot(self.w[1:], x) + self.w[0]
    # return np.dot(np.transpose(self.w),x)
       
    def Id(self,z_): #Identity function - Activation function
        return z_
       
    def learning(self):
        self.cost = []
        for e in range(self.epoch):
            cst = 0
            for k in range(self.tr_inpt.shape[0]):
                
                X = self.tr_inpt[k] #Row k
                Z = self.z(X) #Net input
                
                error = (self.labels[k]-self.Id(Z)) #
                dw = self.Lr*error*X 
                
                self.w[1:] += dw
                self.w[0] += self.Lr*error
                
                cst += .5*(error**2)
               
            self.cost.append(cst)
        
    def quantizer(self, z):
        if z >= 0.0:
            return 1
        else:
            return 0
    
    def testIt(self, testDat, testLabels):
           test_result = []
           right = 0
           for k in range(testDat.shape[0]):
               
               z = self.z(testDat[k])
               prediction = self.quantizer(z)
               
               test_result.append(prediction)
               
               if prediction == testLabels[k]:
                   right += 1
                       
           return (right/len(test_result))*100
       
    def plotErrors_Cost(self):
        costFig = plt.figure() #Initalizes new plot
        plt.title("Number of updates vs Epochs") 
        plt.plot(range(1,len(self.cost)+1), np.log10(self.cost)) #range(1,len(self.updates)+1) is the epochs
        #x = epochs, y = self.updates (number of updates per epoch)
        plt.xlabel('Epochs')
        plt.ylabel("Log(cost)")
        plt.show() #Generates/shows the plot
            
